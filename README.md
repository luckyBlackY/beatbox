私は色々な趣味があります。その中の一つとして **アカペラ** があります。特に **ボイスパーカッション** (以降、ボイパと呼ぶ)を担当していました。ボイパというのは口だけでビートを刻むことを言います。今回はそれをテーマにした自己学習です。しかし、本来達成したい目的を完全には達成できていません。ですので、それを達成するためのアドバイス等いただけると幸いです。

※この記事には音声ファイルが存在します。もし音声を聴く場合は **必ず** イヤホンをして、周りに音が聞こえないようにして下さい。

## 自己学習の内容
大学ではアカペラサークルに入っており、後輩に教えるということもしていました。<br>
その時に、ビートを楽譜に起こして後輩に渡すということもしておりましたが、結構時間がかかります。
#### 口ずさんだビートをそのまま自動で楽譜に起こせたらどれだけ便利なことか
そんなことを思っておりました。

それを思い出し、今回の自己学習のテーマが決まりました。
### 『録音したボイパを自動で楽譜にする機構の作成』
おそらく、処理の流れとしては以下のようになると思っています。

1. 録音したボイパ音声から個々のドラム音を抽出。
2. 抽出した音声をAIモデルに入力し、ドラム音の種類を推論。
3. 推論結果をテキスト形式に変換（例: “b s r…”）。
4. テキストを楽譜アプリで読み取れる形式に変換。

と思っております。

本当は完全にテーマを達成してから紹介しようと思っておりましたが、少し詰まってしまったのとE資格の勉強もあってあまり時間が取れないため、今回は二番目の **何のドラム音かを推測するAIモデル** について、紹介します。

## データセット作成
ボイパでよく出てくる6種類の音を今回は用意します(バスドラム, クローズハイハット, オープンハイハット, スネア, リムショット, クラッシュシンバル)。

用意方法は以下の通りです。

1. 各種の音を出し続けたものを録音する。なるべく様々な音色になるように打ち方を毎回変える。
2. 録音の無音部分で分割し、1つの音声ファイルにつき、1つの音になるように新規保存。

1.の工程では、各6種類の音を4~6分間出し続けたものを録音しました(疲れました)。例として、スネアを5分間出し続けていた音声を添付します(聴く場合は必ずイヤホンして下さい)。

(スネアの音声が入る)

これを以下のスクリプトを用いて、分割した音声を新規保存しました。
```python
from pydub import AudioSegment
from pydub.silence import split_on_silence

# 音声ファイルの読み込み
sound = AudioSegment.from_file("スネア.wav", format="wav")

# 分割の設定
chunks = split_on_silence(
    sound,
    min_silence_len=200,  # 無音とみなす最小の長さ（ミリ秒）
    silence_thresh=-40     # 無音とみなす閾値（dB）
)

# 分割された音声を保存
for i, chunk in enumerate(chunks):
    chunk.export(f"snare/{i}.wav", format="wav")
```

そうして、このように単発音の録音が完成しました。

(単発スネア音が入る)

このようにして、各種類に対して以下の個数のデータセットを作成しました。

| バスドラム | クローズハイハット | オープンハイハット | スネア | リムショット | クラッシュシンバル |
|--|--|--|--|--|--|
| 357 | 382 | 365 | 305 | 449 | 282 |

各種類の音声を学習用:検証用 = 8:2になるように分割したデータセットでモデルの学習を行いました。

各音声は長さがバラバラのため、これを全て1秒になるように調整し、固定サイズにする。そこから、 **メルスペクトログラム** を計算し、二次元配列を作成しました。

メルスペクトログラムは横軸が時間軸, 縦軸が周波数軸, 値がパワーとなっているものです。下記画像は **『FSER: Deep Convolutional Neural Networks for Speech Emotion Recognition』** という論文で紹介されたメルスペクトログラム画像です。この論文はメルスペクトログラムを使い、画像の問題として人間の音声の感情分類をしたというのを紹介している論文です。

(メルスペクトログラムの画像)

今回の自己学習でも同様に、画像の問題として扱い、この画像がどの音かというのを推論するモデルの作成をします。

## モデル学習
モデルは下記のように非常にシンプルな2層の畳み込み層と2層の結合層の構造です。
```python
class CNNModel(nn.Module):
    def __init__(self, num_classes=len(classes), fixed_frames=64):
        super(CNNModel, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.25),

            nn.Conv2d(32, 64, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.25)
        )

        # 全結合層の入力サイズを計算
        self.flatten_input_size = self._get_flatten_size(fixed_frames)

        self.fc_layers = nn.Sequential(
            nn.Linear(self.flatten_input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )

    def _get_flatten_size(self, fixed_frames):
        # ダミー入力を使用してフラット化後のサイズを計算
        with torch.no_grad():
            dummy_input = torch.zeros(1, 1, 128, fixed_frames)
            dummy_output = self.conv_layers(dummy_input)
            flatten_size = dummy_output.view(-1).size(0)
        return flatten_size

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)  # フラット化
        x = self.fc_layers(x)
        return x
```
エポック数は60で、オプティマイザはAdamです。検証用データの損失が一番低いモデルをこの後でも使用することにしました(CPU使用で5.5分という短時間で終了)。

その結果、エポック59のモデルを使用することになりました(下記は50~60エポック時の損失)
```text
Epoch 50/60, Loss: 0.0061
Validation Loss: 0.0052
Epoch 51/60, Loss: 0.0009
Validation Loss: 0.0022
Epoch 52/60, Loss: 0.0052
Validation Loss: 0.0071
Epoch 53/60, Loss: 0.0053
Validation Loss: 0.0053
Epoch 54/60, Loss: 0.0030
Validation Loss: 0.0045
Epoch 55/60, Loss: 0.0036
Validation Loss: 0.0008
Epoch 56/60, Loss: 0.0102
Validation Loss: 0.0011
Epoch 57/60, Loss: 0.0072
Validation Loss: 0.0013
Epoch 58/60, Loss: 0.0022
Validation Loss: 0.0002
Epoch 59/60, Loss: 0.0010
Validation Loss: 0.0001
Epoch 60/60, Loss: 0.0108
Validation Loss: 0.0007
```

ロスが0.0001と非常に小さい値になっているように、検証用データにおいて正しく音声を分類できたものは **100%** でした。

## モデルの実用
先ほどできたモデルを用いて、単音ではなく **実際にビートを刻んだ音声** を入力して、正しくリズムをテキストにできるかを確認しました。

今回のビートはこちらです(聴く場合、イヤホン必須)。

(テスト.wavが入ります)

こちらの音声を先ほどと同様に無音部分を検出して分割し、その単音を一つずつ推論するということを行いました。

テキストの表記は下記のように対応しています。

| バスドラム | クローズハイハット | オープンハイハット | スネア | リムショット | クラッシュシンバル |
|--|--|--|--|--|--|
| b | c-hi | o-hi | s | r | c |

推論結果が上のテキスト, 実際のリズムが下のテキストです。

```text
Predicted sequence: b c-hi b b c-hi b s c-hi b c-hi s r c-hi b s r b c-hi s r c-hi c-hi s c-hi b c-hi s c-hi s c-hi b r c c-hi
```
```text
Actural sequence: b c-hi s b c-hi b s c-hi b c-hi s r c-hi b s r b c-hi s r c-hi c-hi s c-hi s c-hi s c-hi s c-hi s r c
```

スネアをバスドラムと推論してしまっている部分がありますが、ほぼ推論できていると言えます(最後にc-hiが入っていますが、おそらく録音を止めるときの音が原因です)。ちなみにスネアもバスドラムもどちらも破裂音なので、間違えないやすいと考えられます。

このように、単音推論モデルを用いて、ビートをテキストに落とし込むことに成功しました。

## 課題
結論から言いますと、 **このままでは全く実用化できません。**

理由としては、実際のリズムでは、以下の二点が課題としてあります。

- 音と音の間に切れ目がほぼない
- 休符が存在する

実際の音声を聴いて頂いた方がわかりやすいと思うので添付します。なるべく先ほどのリズムに近いものを用意しました(イヤホン必須)。

(テスト_応用.wavが入る)

これを試しに今のやり方と同じ方法で推論しますと、以下のような結果になりました(無音時間はなるべく調整しています)。
```text
Predicted sequence: c c o-hi c-hi c-hi r b o-hi s o-hi c-hi r o-hi r r o-hi r b o-hi o-hi o-hi o-hi o-hi c-hi o-hi r o-hi c-hi c s c-hi b o-hi c-hi o-hi r c-hi r o-hi c-hi o-hi r c-hi r r o-hi c-hi o-hi r r c r c-hi c-hi
```
無音時間の調整をしても、全く正しいリズムを出力することができていません。また、今のやり方だとそもそも無音で分割するため、休符を意識したビートを打っても、それが無視されます。

これらの課題をどうやって解決するかを現在模索中です。色々試してみたのですが、まだうまくいっているものはありません。

下記が試してみた案たちです。
- 波形が急激に変わったタイミングで分割する。
- BPMを自動で検出するモジュールを用いて、そのBPMから細かくタイミング分割し、そのタイミングでなった音を推論。

一つ目の案の場合は、無駄に細かく分割してしまい、先ほどまでできていたシンプルなリズムでさえ上手くいきませんでした。

二つ目の案の場合は、BPMは確かに正しい結果が出ますが、そこからのテキスト出力がうまくいっていないです。理由は、クローズハイハットのように短い音もあればクラッシュのように長い音もあるからです。長い音が分割された時の対処も考えたのですが、現時点では改良できていないです。

そもそも、単音推論モデルでビートを推論すること自体が難しいです。ただ、たくさんの種類のビートに対して、ラベル付けをするのは時間的にあまりにも厳しすぎるので、どうにかして単音推論モデルでやりたいという気持ちはあります。

結局、ここからあまり進捗を埋めず、この自己学習にかけられる時間も限られているため、ここで一旦終了とします。<br>
いい方法があればそれを取り入れて再度進めていけたらと思っております。

## まとめ
メルスペクトログラムを使い、画像の問題として単音を推論するモデルを作成することには成功しました。そして、単純なビートならば、リズムをテキストにすることに成功しました。

一方で、単音推論モデルを用いての複雑なリズムの推論は現時点ではまだうまくいっておりません。こうすれば良さそう等アドバイスあれば教えていただけますと幸いです。

## 余談
2024年ももうすぐ終わるということで、来週は自己学習で進めようと思ったものの途中でボツにしてしまった二つをプチ紹介しようと思います。